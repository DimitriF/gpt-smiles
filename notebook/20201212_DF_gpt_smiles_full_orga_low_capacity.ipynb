{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 170\n",
    "# max_len = 20\n",
    "batch_size = 64\n",
    "n_layer = 8 \n",
    "n_head = 4\n",
    "n_embd = 256\n",
    "max_epochs = 20\n",
    "\n",
    "stop_token = '$'\n",
    "split_token = '&'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/12/2020 21:11:49 - INFO - rdkit -   Enabling RDKit 2020.09.1 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "\n",
    "# from flattenV3Advanced import flattenData\n",
    "\n",
    "# make deterministic\n",
    "from minGPT.mingpt.utils import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = torch.cuda.current_device()\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "from minGPT.mingpt.model import GPT, GPTConfig\n",
    "# from minGPT.mingpt.trainer import Trainer, TrainerConfig\n",
    "# from minGPT.mingpt.utils import sample\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from rdkit.Chem.rdMolDescriptors import CalcMolFormula\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "from rdkit import Chem\n",
    "from rdkit.rdBase import BlockLogs\n",
    "from rdkit.Chem import PandasTools\n",
    "block = BlockLogs() ## not sure we want to block all but rdkit complain when wrong smiles are sent...\n",
    "\n",
    "## dataset path\n",
    "data_path = '/home/teleport/perso/10pc_cleaned_small.csv'\n",
    "EXPERIMENT = '20201212_DF_gpt_smiles_full_orga_low_capacity'\n",
    "experiment_path = \"/opt/data/train_dir/\" + EXPERIMENT + \"/\"\n",
    "if not os.path.exists(experiment_path):\n",
    "    os.mkdir(experiment_path)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trains and tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINS Task: overwriting (reusing) task id=91b7d9a891de4ef4a1de0c1c707eb161\n",
      "TRAINS results page: http://172.22.4.157:8080/projects/8f2ac9400a46477683280746b293e09f/experiments/91b7d9a891de4ef4a1de0c1c707eb161/output/log\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "writer = SummaryWriter(experiment_path + '/' + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\")\n",
    "from trains import Task\n",
    "import trains\n",
    "task = Task.init(project_name='gpt_smiles', task_name=EXPERIMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-12 21:11:51,457 - trains.Repository Detection - WARNING - Can't get branch information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-12 21:11:51,467 - trains.Repository Detection - WARNING - Can't get commit information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-12 21:11:51,477 - trains.Repository Detection - WARNING - Can't get root information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-12 21:11:51,487 - trains.Repository Detection - WARNING - Can't get status information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-12 21:11:51,540 - trains.Repository Detection - WARNING - Can't get diff information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-12 21:11:51,551 - trains.Repository Detection - WARNING - Can't get modified information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "TRAINS new version available: upgrade to v0.16.4 is recommended!\n",
      "18245323\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(data_path)\n",
    "# data = data.sample(frac = 0.2) ## already sampled...\n",
    "data.columns\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv(data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18245323\n",
      "18217603\n",
      "27720\n"
     ]
    }
   ],
   "source": [
    "data_train = data[data['train'] == True].reset_index()\n",
    "data_test = data[data['train'] == False].reset_index()\n",
    "print(len(data))\n",
    "print(len(data_train))\n",
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['C','c','N','n','O','o','H','B','r','l','I','F','S','s','P','.','[',']','(',')','-','+','=','@','/','\\\\','#','$','&']\n",
    "word_list += list(string.digits)\n",
    "stoi = { ch:i for i,ch in enumerate(word_list) }\n",
    "itos = { i:ch for i,ch in enumerate(word_list) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for checking the smiles and tokenize them, detokenize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_idx(data, idx, stoi, block_size = None, stop_token = '$', split_token = '&'):\n",
    "    return tokenize_smiles(data['smiles'][idx],data['MolFormula'][idx],data['logp'][idx], stoi, block_size=block_size, stop_token = '$', split_token = '&')\n",
    "\n",
    "def tokenize_smiles(smiles, molecular_formula, logp, stoi, block_size = None, stop_token = '$', split_token = '&'):\n",
    "    chunk = str(round(logp,1)) + split_token + molecular_formula + split_token + smiles + stop_token\n",
    "    if block_size is not None:\n",
    "        chunk = chunk + stop_token*(block_size - len(chunk))\n",
    "    data = [stoi[s] for s in chunk]\n",
    "    return data\n",
    "\n",
    "def y_to_completion(y, itos):\n",
    "    return ''.join([itos[int(i)] for i in y])\n",
    "    \n",
    "\n",
    "# def check_completion(completion, stop_token = '$', split_token = '&'):\n",
    "#     '''\n",
    "#     return a string with the status of the check:\n",
    "#     full_failure: cannot process it at all\n",
    "#     smiles_failure: MolFromSmiles return None\n",
    "#     MolFormula_failure: CalcMolFormula from the smile is different\n",
    "#     success: CalcMolFormula from the smile is same\n",
    "#     '''\n",
    "    \n",
    "#     try:\n",
    "#         completion = completion.split(split_token)\n",
    "#         logp = completion[0]\n",
    "#         MolFormula = completion[1]\n",
    "#         smiles = completion[2].split(stop_token)[0]\n",
    "#         mol = Chem.MolFromSmiles(smiles)\n",
    "#         if mol is None:\n",
    "#             return 'smiles_failure'\n",
    "#         MolFormula_smiles = CalcMolFormula(mol)\n",
    "#         logp_smiles = Descriptors.MolLogP(mol)\n",
    "#         if MolFormula != MolFormula_smiles:\n",
    "#             return 'MolFormula_failure'\n",
    "#         return 'success'\n",
    "#     except:\n",
    "#         return 'full_failure'\n",
    "\n",
    "    \n",
    "def check_model_output(model, MolFormulas, logps, stop_token = '$', split_token = '&', batch_size = 4,temperature=0.9):\n",
    "    completions = []\n",
    "    for i, j  in zip(MolFormulas, logps):        \n",
    "        data = [stoi[s] for s in str(round(j,1)) + split_token + i + split_token]\n",
    "        x = [torch.tensor(data, dtype=torch.long).unsqueeze(0).to(device)]\n",
    "        y = sample_clean(model, x, steps = block_size, temperature=temperature, sample=True, top_k=5, itos = itos, stop_token = '$', batch_size = batch_size)\n",
    "        for idx in range(y.size(0)):    \n",
    "            completion = y_to_completion(y[idx], itos)\n",
    "            completions.append(completion)\n",
    "    df = completion_to_pandas(completions)\n",
    "            \n",
    "    return df, completions\n",
    "    \n",
    "def completion_to_pandas(completion):\n",
    "    smiles = []\n",
    "    MolFormulaInput = []\n",
    "    MolFormulaOutput = []\n",
    "    logpInput = []\n",
    "    logpOutput = []\n",
    "    for i in completion:\n",
    "#         print(i)\n",
    "        i = i.split(split_token)\n",
    "        logpInput.append(i[0])\n",
    "        MolFormulaInput.append(i[1])\n",
    "        smile = i[2].split(stop_token)[0]\n",
    "        smiles.append(smile)\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        if mol is None:\n",
    "            MolFormulaOutput.append('fail')\n",
    "            logpOutput.append('fail')\n",
    "        else:\n",
    "            MolFormulaOutput.append(CalcMolFormula(mol))\n",
    "            logpOutput.append(Descriptors.MolLogP(mol))\n",
    "    df = pd.DataFrame(list(zip(logpInput,MolFormulaInput, smiles,logpOutput, MolFormulaOutput)), \n",
    "               columns =['logpInput','MolFormulaInput', 'smiles', 'logpOutput', 'MolFormulaOutput']) \n",
    "    PandasTools.AddMoleculeColumnToFrame(df,smilesCol='smiles')\n",
    "    return(df)\n",
    "\n",
    "def check_df(df):\n",
    "    df2 = df[df['MolFormulaOutput'] != 'fail']\n",
    "    smiles_failure_ratio = (len(df) - len(df2))/len(df)\n",
    "    MolFormula_success = sum(df2['MolFormulaInput'] == df2['MolFormulaOutput'])/len(df)\n",
    "    MolFormula_failure = sum(df2['MolFormulaInput'] != df2['MolFormulaOutput'])/len(df)\n",
    "    logp_mse = mean_squared_error(df2['logpInput'],df2['logpOutput'])\n",
    "    return {'smiles_failure_ratio':smiles_failure_ratio, \n",
    "            'MolFormula_failure':MolFormula_failure,\n",
    "           'MolFormula_success':MolFormula_success,\n",
    "           'logp_mse':logp_mse}\n",
    "\n",
    "\n",
    "def add_mol(writer, smiles, global_step=None, size=(300, 300)):\n",
    "    \"\"\"\n",
    "    Adds a molecule to the images section of Tensorboard.\n",
    "    \"\"\"\n",
    "    img_transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    image = Chem.Draw.MolToImage(mol, size=size)\n",
    "    writer.add_image(smiles, img_transform(image), global_step) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smiles_failure_ratio': 0.0,\n",
       " 'MolFormula_failure': 0.0,\n",
       " 'MolFormula_success': 1.0,\n",
       " 'logp_mse': 1.4440000000020445e-05}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tokenize_idx(data_test, 0, stoi)\n",
    "completion = y_to_completion(y, itos)\n",
    "completion\n",
    "\n",
    "check_df(completion_to_pandas([completion]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smiles_failure_ratio': 0.0,\n",
       " 'MolFormula_failure': 0.0,\n",
       " 'MolFormula_success': 1.0,\n",
       " 'logp_mse': 0.26863489000000185}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = '1.2&C23H39N5O&CN(C)c1ccc(CNC(=O)CN2CCC(C)(CN3CCN(C)CC3)CC2)cc1'\n",
    "check_df(completion_to_pandas([completion]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmilesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size, stoi, itos):\n",
    "        \n",
    "        self.data = data\n",
    "        self.stoi = stoi\n",
    "        self.itos = itos\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = len(self.stoi)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx,verbose=False):\n",
    "        try:\n",
    "            converted_chunk = tokenize_idx(self.data, idx, self.stoi, self.block_size)\n",
    "        except:\n",
    "            print('error in data loader: idx = ' + idx)\n",
    "            return self.__getitem__(random.randint(0, len(self.data)))\n",
    "        x = torch.tensor(converted_chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(converted_chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "            \n",
    "train_dataset = SmilesDataset(data_train, block_size, stoi, itos) \n",
    "test_dataset = SmilesDataset(data_test, block_size, stoi, itos) \n",
    "\n",
    "df = train_dataset.__getitem__(idx = 10,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/12/2020 21:12:12 - INFO - minGPT.mingpt.model -   number of parameters: 6.382080e+06\n"
     ]
    }
   ],
   "source": [
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=n_layer, n_head=n_head, n_embd=n_embd)\n",
    "model = GPT(mconf)\n",
    "# model.load_state_dict(torch.load(experiment_path + 'model_0.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[:, [-1]]] = -float('Inf')\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_clean(model, xs, steps, temperature=1.0, sample=False, top_k=None, itos = None, stop_token = '$',batch_size = 4):\n",
    "#     block_size = model.get_block_size()\n",
    "    model.eval()\n",
    "    xs = sorted(xs, key = lambda x: x.size(1))\n",
    "    ## add the batch size\n",
    "    xs = [torch.cat(batch_size*[x]) for x in xs]\n",
    "    \n",
    "    ## start by making them the same length\n",
    "    x = xs[0]\n",
    "    for i in range(1,len(xs)): # nothing to do if length 1\n",
    "        while x.size(1) < xs[i].size(1):\n",
    "            x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
    "            logits, _ = model(x_cond)\n",
    "            # pluck the logits at the final step and scale by temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop probabilities to only the top k options\n",
    "            if top_k is not None:\n",
    "                logits = top_k_logits(logits, top_k)\n",
    "            # apply softmax to convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution or take the most likely\n",
    "            if sample:\n",
    "                ix = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "            # append to the sequence and continue\n",
    "            x = torch.cat((x, ix), dim=1)\n",
    "        x= torch.cat((x,xs[i]), dim=0)\n",
    "    \n",
    "    \n",
    "    ## now we can keep going...\n",
    "    keep_going = True\n",
    "    while keep_going:\n",
    "#         print(x.size(1))\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
    "        logits, _ = model(x_cond)\n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "        # append to the sequence and continue\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "        if x.size(1) == block_size:\n",
    "            keep_going = False\n",
    "        if itos is not None:\n",
    "            sequences_over = 0\n",
    "            for i in range(x.size(0)):                \n",
    "                if itos[int(ix[i])] == stop_token:\n",
    "                    sequences_over += 1\n",
    "            if sequences_over == x.size(0):\n",
    "                keep_going = False\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    batch_size = 64\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False\n",
    "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = None\n",
    "    num_workers = 0 # for DataLoader\n",
    "    test_every_iteration = 50000\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_dataset, test_dataset, config):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.config = config\n",
    "        MolFormulas = list(self.test_dataset.data['MolFormula'].unique())\n",
    "        logps = [test_dataset.data[test_dataset.data['MolFormula'] == i] for i in MolFormulas]\n",
    "        logps = [list(i['logp'])[0] for i in logps]\n",
    "        self.MolFormulas = MolFormulas\n",
    "        self.logps = logps\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        logger.info(\"saving %s\", self.config.ckpt_path)\n",
    "        torch.save(raw_model.state_dict(), self.config.ckpt_path + 'model.pt')\n",
    "    \n",
    "    def perform_test(self, it, epoch):\n",
    "        x = (epoch) * len(self.train_dataset) + it * self.config.batch_size\n",
    "        test_loss = self.run_epoch('test', epoch)\n",
    "        writer.add_scalar('loss/test_loss',test_loss,x)\n",
    "        df, completions = check_model_output(model, self.MolFormulas, self.logps, batch_size = 16)  \n",
    "        results = check_df(df)\n",
    "        for i in ['MolFormula_success','MolFormula_failure','smiles_failure_ratio']:\n",
    "            writer.add_scalar('results/'+i,results[i],x)\n",
    "        writer.add_scalar('results_logp/'+'logp_mse',results['logp_mse'],x)\n",
    "        df2 = df[df['MolFormulaOutput'] != 'fail'].sample(n=20)\n",
    "#         for i  in list(df2['smiles']):\n",
    "#             add_mol(writer, i, i, x)\n",
    "        \n",
    "        \n",
    "    def run_epoch(self,split, epoch):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "        \n",
    "        is_train = split == 'train'\n",
    "        model.train(is_train)\n",
    "        data = self.train_dataset if is_train else self.test_dataset\n",
    "        loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
    "                            batch_size=config.batch_size,\n",
    "                            num_workers=config.num_workers)\n",
    "\n",
    "        losses = []\n",
    "        # pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "        for it, (x, y) in enumerate(loader):\n",
    "\n",
    "            # place data on the correct device\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            # forward the model\n",
    "            with torch.set_grad_enabled(is_train):\n",
    "                logits, loss = model(x, y)\n",
    "                loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            if is_train:\n",
    "\n",
    "                # backprop and update the parameters\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                optimizer.step()\n",
    "\n",
    "                # decay the learning rate based on our progress\n",
    "                if config.lr_decay:\n",
    "                    self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "                    if self.tokens < config.warmup_tokens:\n",
    "                        # linear warmup\n",
    "                        lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                    else:\n",
    "                        # cosine learning rate decay\n",
    "                        progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                        lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                    lr = config.learning_rate * lr_mult\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] = lr\n",
    "                else:\n",
    "                    lr = config.learning_rate\n",
    "\n",
    "                # report progress\n",
    "                # pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
    "\n",
    "                # perform periodic test\n",
    "                if (it+1) % config.test_every_iteration == 0 and self.test_dataset is not None:\n",
    "                    self.perform_test(it=it,epoch=epoch)\n",
    "\n",
    "\n",
    "        if not is_train:\n",
    "            test_loss = float(np.mean(losses))\n",
    "            logger.info(\"test loss: %f\", test_loss)\n",
    "            return test_loss\n",
    "        else:\n",
    "            train_loss = float(np.mean(losses))\n",
    "            return train_loss\n",
    "            \n",
    "    def train(self, writer):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "\n",
    "        \n",
    "\n",
    "        best_loss = float('inf')\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "        for epoch in range(config.max_epochs):\n",
    "\n",
    "            train_loss = self.run_epoch('train', epoch)\n",
    "            x = (epoch+1) * len(self.train_dataset)\n",
    "            writer.add_scalar('loss/train_loss',train_loss,x)\n",
    "#             if self.test_dataset is not None:\n",
    "#                 self.perform_test(it=0,epoch=epoch+1)\n",
    "\n",
    "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
    "#             good_model = self.test_dataset is None or test_loss < best_loss\n",
    "#             if self.config.ckpt_path is not None and good_model:\n",
    "#                 best_loss = test_loss\n",
    "            self.save_checkpoint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINS Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/12/2020 21:28:44 - INFO - __main__ -   test loss: 0.193556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINS Monitor: Reporting detected, reverting back to iteration based reporting\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=max_epochs, batch_size=batch_size, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(train_dataset)*block_size,\n",
    "                      num_workers=8, ckpt_path = experiment_path,\n",
    "                     test_every_iteration = 10000) ## 10000 * 128 = 1M approx\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "trainer.train(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.perform_test(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, completion = check_model_output(model,['C16H32N4O2'],[1.0],batch_size=16,temperature=2)\n",
    "df = completion_to_pandas(completion)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = 'CCOC(=O)c1ccc(O[C@H]2CCC[C@](C)(OCC)CC2)c(OCC2CCOCC2)c1'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_smiles",
   "language": "python",
   "name": "pytorch_smiles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
