{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 170\n",
    "# max_len = 20\n",
    "batch_size = 128\n",
    "n_layer = 8 \n",
    "n_head = 4\n",
    "n_embd = 256\n",
    "max_epochs = 20\n",
    "\n",
    "test_every_iteration = 20000\n",
    "\n",
    "stop_token = '$'\n",
    "split_token = '&'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/15/2020 23:32:20 - INFO - rdkit -   Enabling RDKit 2020.09.1 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "\n",
    "# from flattenV3Advanced import flattenData\n",
    "\n",
    "# make deterministic\n",
    "from minGPT.mingpt.utils import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = torch.cuda.current_device()\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "from minGPT.mingpt.model import GPT, GPTConfig\n",
    "# from minGPT.mingpt.trainer import Trainer, TrainerConfig\n",
    "# from minGPT.mingpt.utils import sample\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from rdkit.Chem.rdMolDescriptors import CalcMolFormula\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "from rdkit import Chem\n",
    "from rdkit.rdBase import BlockLogs\n",
    "from rdkit.Chem import PandasTools\n",
    "block = BlockLogs() ## not sure we want to block all but rdkit complain when wrong smiles are sent...\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "## dataset path\n",
    "data_path = '/home/teleport/perso/smiles_clean.db'\n",
    "EXPERIMENT = '20201215_DF_gpt_smiles_full_orga_low_capacity'\n",
    "experiment_path = \"/opt/data/train_dir/\" + EXPERIMENT + \"/\"\n",
    "if not os.path.exists(experiment_path):\n",
    "    os.mkdir(experiment_path)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trains and tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINS Task: overwriting (reusing) task id=8fd75fddb8774a4fa5941cd22336bb6e\n",
      "TRAINS results page: http://172.22.4.157:8080/projects/8f2ac9400a46477683280746b293e09f/experiments/8fd75fddb8774a4fa5941cd22336bb6e/output/log\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "writer = SummaryWriter(experiment_path + '/' + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\")\n",
    "from trains import Task\n",
    "import trains\n",
    "task = Task.init(project_name='gpt_smiles', task_name=EXPERIMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created and Successfully Connected to SQLite\n"
     ]
    }
   ],
   "source": [
    "sqliteConnection = sqlite3.connect(data_path)\n",
    "cursor = sqliteConnection.cursor()\n",
    "print(\"Database created and Successfully Connected to SQLite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-15 23:32:21,202 - trains.Repository Detection - WARNING - Can't get branch information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-15 23:32:21,213 - trains.Repository Detection - WARNING - Can't get commit information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-15 23:32:21,224 - trains.Repository Detection - WARNING - Can't get root information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-15 23:32:21,234 - trains.Repository Detection - WARNING - Can't get status information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-15 23:32:21,256 - trains.Repository Detection - WARNING - Can't get diff information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-15 23:32:21,267 - trains.Repository Detection - WARNING - Can't get modified information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(91161254,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('SELECT COUNT() FROM smiles_train')\n",
    "n_train = cursor.fetchone()\n",
    "n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162554,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('SELECT COUNT() FROM smiles_test')\n",
    "n_test = cursor.fetchone()\n",
    "n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolFormula</th>\n",
       "      <th>train</th>\n",
       "      <th>logp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C20H24FN3O2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C18H17NO5S</td>\n",
       "      <td>0</td>\n",
       "      <td>2.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C15H18F3NO4S</td>\n",
       "      <td>0</td>\n",
       "      <td>2.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C15H15F3N4O3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C16H13ClN6O2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>C31H33F2N3O6S</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>C15H21N9O5</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>C12H5F4IN2O</td>\n",
       "      <td>0</td>\n",
       "      <td>3.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>C12H13FINOS2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>C12H6BrF4N5O</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1139 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         MolFormula  train   logp\n",
       "0       C20H24FN3O2      0  2.800\n",
       "1        C18H17NO5S      0  2.994\n",
       "2      C15H18F3NO4S      0  2.860\n",
       "3      C15H15F3N4O3      0  2.748\n",
       "4      C16H13ClN6O2      0  2.603\n",
       "...             ...    ...    ...\n",
       "1134  C31H33F2N3O6S      0  1.000\n",
       "1135     C15H21N9O5      0 -1.537\n",
       "1136    C12H5F4IN2O      0  3.495\n",
       "1137   C12H13FINOS2      0  1.000\n",
       "1138   C12H6BrF4N5O      0  1.000\n",
       "\n",
       "[1139 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('SELECT *  FROM MolFormulas WHERE train = 0')\n",
    "MolFormulas_test = cursor.fetchall()\n",
    "MolFormulas_test = pd.DataFrame(MolFormulas_test, columns = ['MolFormula','train','logp'])\n",
    "MolFormulas_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the logps and mol formulas for testing\n",
    "MolFormulas = list(MolFormulas_test['MolFormula'])\n",
    "logps = list(MolFormulas_test['logp'])\n",
    "# logps = [list(i['logp'])[0] for i in logps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (sqliteConnection):\n",
    "    sqliteConnection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['C','c','N','n','O','o','H','B','r','l','I','F','S','s','P','.','[',']','(',')','-','+','=','@','/','\\\\','#','$','&']\n",
    "word_list += list(string.digits)\n",
    "stoi = { ch:i for i,ch in enumerate(word_list) }\n",
    "itos = { i:ch for i,ch in enumerate(word_list) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for checking the smiles and tokenize them, detokenize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('asdpajsdpsd'[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_idx(data, idx, stoi, block_size = None, stop_token = '$', split_token = '&'):\n",
    "#     return tokenize_smiles(data['smiles'][idx],data['MolFormula'][idx],data['logp'][idx], stoi, block_size=block_size, stop_token = '$', split_token = '&')\n",
    "\n",
    "def tokenize_smiles(smiles, molecular_formula, logp, stoi, block_size = None, stop_token = '$', split_token = '&'):\n",
    "    chunk = str(round(logp,1)) + split_token + molecular_formula + split_token + smiles + stop_token\n",
    "    if block_size is not None:\n",
    "        if len(chunk) < block_size:\n",
    "            chunk = chunk + stop_token*(block_size - len(chunk))\n",
    "        elif len(chunk) > block_size:\n",
    "            chunk = chunk[:block_size]\n",
    "    data = [stoi[s] for s in chunk]\n",
    "    return data\n",
    "\n",
    "def y_to_completion(y, itos):\n",
    "    return ''.join([itos[int(i)] for i in y])\n",
    "    \n",
    "\n",
    "# def check_completion(completion, stop_token = '$', split_token = '&'):\n",
    "#     '''\n",
    "#     return a string with the status of the check:\n",
    "#     full_failure: cannot process it at all\n",
    "#     smiles_failure: MolFromSmiles return None\n",
    "#     MolFormula_failure: CalcMolFormula from the smile is different\n",
    "#     success: CalcMolFormula from the smile is same\n",
    "#     '''\n",
    "    \n",
    "#     try:\n",
    "#         completion = completion.split(split_token)\n",
    "#         logp = completion[0]\n",
    "#         MolFormula = completion[1]\n",
    "#         smiles = completion[2].split(stop_token)[0]\n",
    "#         mol = Chem.MolFromSmiles(smiles)\n",
    "#         if mol is None:\n",
    "#             return 'smiles_failure'\n",
    "#         MolFormula_smiles = CalcMolFormula(mol)\n",
    "#         logp_smiles = Descriptors.MolLogP(mol)\n",
    "#         if MolFormula != MolFormula_smiles:\n",
    "#             return 'MolFormula_failure'\n",
    "#         return 'success'\n",
    "#     except:\n",
    "#         return 'full_failure'\n",
    "\n",
    "    \n",
    "def check_model_output(model, MolFormulas, logps, stop_token = '$', split_token = '&', batch_size = 4,temperature=0.9):\n",
    "    completions = []\n",
    "    for i, j  in zip(MolFormulas, logps):        \n",
    "        data = [stoi[s] for s in str(round(j,1)) + split_token + i + split_token]\n",
    "        x = [torch.tensor(data, dtype=torch.long).unsqueeze(0).to(device)]\n",
    "        y = sample_clean(model, x, steps = block_size, temperature=temperature, sample=True, top_k=5, itos = itos, stop_token = '$', batch_size = batch_size)\n",
    "        for idx in range(y.size(0)):    \n",
    "            completion = y_to_completion(y[idx], itos)\n",
    "            completions.append(completion)\n",
    "    df = completion_to_pandas(completions)\n",
    "            \n",
    "    return df, completions\n",
    "    \n",
    "def completion_to_pandas(completion):\n",
    "    smiles = []\n",
    "    MolFormulaInput = []\n",
    "    MolFormulaOutput = []\n",
    "    logpInput = []\n",
    "    logpOutput = []\n",
    "    for i in completion:\n",
    "#         print(i)\n",
    "        i = i.split(split_token)\n",
    "        logpInput.append(i[0])\n",
    "        MolFormulaInput.append(i[1])\n",
    "        smile = i[2].split(stop_token)[0]\n",
    "        smiles.append(smile)\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        if mol is None:\n",
    "            MolFormulaOutput.append('fail')\n",
    "            logpOutput.append('fail')\n",
    "        else:\n",
    "            MolFormulaOutput.append(CalcMolFormula(mol))\n",
    "            logpOutput.append(Descriptors.MolLogP(mol))\n",
    "    df = pd.DataFrame(list(zip(logpInput,MolFormulaInput, smiles,logpOutput, MolFormulaOutput)), \n",
    "               columns =['logpInput','MolFormulaInput', 'smiles', 'logpOutput', 'MolFormulaOutput']) \n",
    "    PandasTools.AddMoleculeColumnToFrame(df,smilesCol='smiles')\n",
    "    return(df)\n",
    "\n",
    "def check_df(df):\n",
    "    df2 = df[df['MolFormulaOutput'] != 'fail']\n",
    "    smiles_failure_ratio = (len(df) - len(df2))/len(df)\n",
    "    MolFormula_success = sum(df2['MolFormulaInput'] == df2['MolFormulaOutput'])/len(df)\n",
    "    MolFormula_failure = sum(df2['MolFormulaInput'] != df2['MolFormulaOutput'])/len(df)\n",
    "    logp_mse = mean_squared_error(df2['logpInput'],df2['logpOutput'])\n",
    "    return {'smiles_failure_ratio':smiles_failure_ratio, \n",
    "            'MolFormula_failure':MolFormula_failure,\n",
    "           'MolFormula_success':MolFormula_success,\n",
    "           'logp_mse':logp_mse}\n",
    "\n",
    "\n",
    "def add_mol(writer, smiles, global_step=None, size=(300, 300)):\n",
    "    \"\"\"\n",
    "    Adds a molecule to the images section of Tensorboard.\n",
    "    \"\"\"\n",
    "    img_transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    image = Chem.Draw.MolToImage(mol, size=size)\n",
    "    writer.add_image(smiles, img_transform(image), global_step) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = tokenize_idx(data_test, 0, stoi)\n",
    "# completion = y_to_completion(y, itos)\n",
    "# completion\n",
    "\n",
    "# check_df(completion_to_pandas([completion]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smiles_failure_ratio': 0.0,\n",
       " 'MolFormula_failure': 0.0,\n",
       " 'MolFormula_success': 1.0,\n",
       " 'logp_mse': 0.26863489000000185}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = '1.2&C23H39N5O&CN(C)c1ccc(CNC(=O)CN2CCC(C)(CN3CCN(C)CC3)CC2)cc1'\n",
    "check_df(completion_to_pandas([completion]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([32, 15, 33, 28,  0, 31, 31,  6, 32, 32,  2,  4, 33, 28,  0,  0, 18,  0,\n",
       "         19, 18,  0, 19,  1, 30,  1,  1,  1,  1,  1, 30,  4,  0,  0, 18, 22,  4,\n",
       "         19,  2, 30,  0,  0,  0, 18,  4,  0, 16,  0, 23, 23,  6, 17, 31,  0,  0,\n",
       "          4,  0, 31, 19,  0,  0, 30, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27]),\n",
       " tensor([15, 33, 28,  0, 31, 31,  6, 32, 32,  2,  4, 33, 28,  0,  0, 18,  0, 19,\n",
       "         18,  0, 19,  1, 30,  1,  1,  1,  1,  1, 30,  4,  0,  0, 18, 22,  4, 19,\n",
       "          2, 30,  0,  0,  0, 18,  4,  0, 16,  0, 23, 23,  6, 17, 31,  0,  0,  4,\n",
       "          0, 31, 19,  0,  0, 30, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SmilesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, table, n, id_string, block_size, stoi, itos):\n",
    "        \n",
    "        self.table = table\n",
    "        self.n = n\n",
    "        self.id_string = id_string\n",
    "        self.stoi = stoi\n",
    "        self.itos = itos\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = len(self.stoi)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx,verbose=False):\n",
    "        try:\n",
    "            sqliteConnection = sqlite3.connect(data_path)\n",
    "            cursor = sqliteConnection.cursor()\n",
    "            sqlite_select_Query = \"SELECT * FROM \" + self.table + \"  WHERE \" + self.id_string + \" = \" + str(idx+1)\n",
    "            cursor.execute(sqlite_select_Query)\n",
    "            record = cursor.fetchone()\n",
    "            if (sqliteConnection):\n",
    "                sqliteConnection.close()\n",
    "#             print(record[5])\n",
    "            smiles = record[1]\n",
    "            molecular_formula = record[5]\n",
    "            logp = record[3]\n",
    "            converted_chunk = tokenize_smiles(smiles, molecular_formula, logp, self.stoi, self.block_size)\n",
    "        except:\n",
    "            print('error in data loader: idx = ' + str(idx))\n",
    "            try:\n",
    "                print(record)\n",
    "            except:\n",
    "                print('could not even print record')\n",
    "            return self.__getitem__(random.randint(0, self.n))\n",
    "        x = torch.tensor(converted_chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(converted_chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "            \n",
    "train_dataset = SmilesDataset('smiles_train', n_train[0], 'id', block_size, stoi, itos) \n",
    "test_dataset = SmilesDataset('smiles_test', n_test[0], 'id', block_size, stoi, itos) \n",
    "\n",
    "df = train_dataset.__getitem__(idx = 91028281,verbose=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqliteConnection = sqlite3.connect(data_path)\n",
    "# cursor = sqliteConnection.cursor()\n",
    "# sqlite_select_Query = \"SELECT * FROM \" + 'smiles_train' + \"  WHERE \" + 'id' + \" = \" + str(61678207+1)\n",
    "# cursor.execute(sqlite_select_Query)\n",
    "# record = cursor.fetchone()\n",
    "# print(record)\n",
    "# if (sqliteConnection):\n",
    "#     sqliteConnection.close()\n",
    "# #             print(record[5])\n",
    "# smiles = record[1]\n",
    "# molecular_formula = record[5]\n",
    "# logp = record[3]\n",
    "# # converted_chunk = tokenize_smiles(smiles, molecular_formula, logp, self.stoi, self.block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINS new version available: upgrade to v0.16.4 is recommended!\n"
     ]
    }
   ],
   "source": [
    "# sqliteConnection = sqlite3.connect('/home/teleport/perso/smiles.db')\n",
    "# cursor = sqliteConnection.cursor()\n",
    "# sqlite_select_Query = \"SELECT * FROM \" + 'smiles_test' + \"  WHERE \" + 'id' + \" = \" + str(1)\n",
    "# cursor.execute(sqlite_select_Query)\n",
    "# record = cursor.fetchone()\n",
    "# print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/15/2020 23:32:21 - INFO - minGPT.mingpt.model -   number of parameters: 6.382080e+06\n"
     ]
    }
   ],
   "source": [
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=n_layer, n_head=n_head, n_embd=n_embd)\n",
    "model = GPT(mconf)\n",
    "# model.load_state_dict(torch.load(experiment_path + 'model_0.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[:, [-1]]] = -float('Inf')\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_clean(model, xs, steps, temperature=1.0, sample=False, top_k=None, itos = None, stop_token = '$',batch_size = 4):\n",
    "#     block_size = model.get_block_size()\n",
    "    model.eval()\n",
    "    xs = sorted(xs, key = lambda x: x.size(1))\n",
    "    ## add the batch size\n",
    "    xs = [torch.cat(batch_size*[x]) for x in xs]\n",
    "    \n",
    "    ## start by making them the same length\n",
    "    x = xs[0]\n",
    "    for i in range(1,len(xs)): # nothing to do if length 1\n",
    "        while x.size(1) < xs[i].size(1):\n",
    "            x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
    "            logits, _ = model(x_cond)\n",
    "            # pluck the logits at the final step and scale by temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop probabilities to only the top k options\n",
    "            if top_k is not None:\n",
    "                logits = top_k_logits(logits, top_k)\n",
    "            # apply softmax to convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution or take the most likely\n",
    "            if sample:\n",
    "                ix = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "            # append to the sequence and continue\n",
    "            x = torch.cat((x, ix), dim=1)\n",
    "        x= torch.cat((x,xs[i]), dim=0)\n",
    "    \n",
    "    \n",
    "    ## now we can keep going...\n",
    "    keep_going = True\n",
    "    while keep_going:\n",
    "#         print(x.size(1))\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
    "        logits, _ = model(x_cond)\n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "        # append to the sequence and continue\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "        if x.size(1) == block_size:\n",
    "            keep_going = False\n",
    "        if itos is not None:\n",
    "            sequences_over = 0\n",
    "            for i in range(x.size(0)):                \n",
    "                if itos[int(ix[i])] == stop_token:\n",
    "                    sequences_over += 1\n",
    "            if sequences_over == x.size(0):\n",
    "                keep_going = False\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    batch_size = 64\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False\n",
    "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = None\n",
    "    num_workers = 0 # for DataLoader\n",
    "    test_every_iteration = 50000\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_dataset, test_dataset, config):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.config = config\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        logger.info(\"saving %s\", self.config.ckpt_path)\n",
    "        torch.save(raw_model.state_dict(), self.config.ckpt_path + 'model.pt')\n",
    "\n",
    "    def train(self, writer):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "\n",
    "        def run_epoch(split, epoch):\n",
    "            is_train = split == 'train'\n",
    "            model.train(is_train)\n",
    "            data = self.train_dataset if is_train else self.test_dataset\n",
    "            loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
    "                                batch_size=config.batch_size,\n",
    "                                num_workers=config.num_workers)\n",
    "\n",
    "            losses = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "            for it, (x, y) in pbar:\n",
    "\n",
    "                # place data on the correct device\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                # forward the model\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    logits, loss = model(x, y)\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "\n",
    "                if is_train:\n",
    "\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # decay the learning rate based on our progress\n",
    "                    if config.lr_decay:\n",
    "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "                        if self.tokens < config.warmup_tokens:\n",
    "                            # linear warmup\n",
    "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                        else:\n",
    "                            # cosine learning rate decay\n",
    "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                        lr = config.learning_rate * lr_mult\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                    else:\n",
    "                        lr = config.learning_rate\n",
    "\n",
    "                    # report progress\n",
    "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
    "                    \n",
    "                    ## check if iterations is modulo of iteration interval and performa  test:\n",
    "                    \n",
    "                    if (it+1) % config.test_every_iteration == 0 and self.test_dataset is not None:\n",
    "                        test_loss = run_epoch('test', epoch)\n",
    "                        x = (epoch) * len(self.train_dataset) + it * self.config.batch_size\n",
    "                        writer.add_scalar('loss/test_loss',test_loss,x)\n",
    "                        \n",
    "                        MolFormulas = self.config.MolFormulas\n",
    "                        logps = self.config.logps\n",
    "                        df, completions = check_model_output(model, MolFormulas, logps, batch_size = 16)  \n",
    "                        results = check_df(df)\n",
    "                        for i in ['MolFormula_success','MolFormula_failure','smiles_failure_ratio']:\n",
    "                            writer.add_scalar('results/'+i,results[i],x)\n",
    "                        writer.add_scalar('results_logp/'+'logp_mse',results['logp_mse'],x)\n",
    "                        self.save_checkpoint()\n",
    "                    \n",
    "\n",
    "            if not is_train:\n",
    "                test_loss = float(np.mean(losses))\n",
    "                logger.info(\"test loss: %f\", test_loss)\n",
    "                return test_loss\n",
    "            else:\n",
    "                train_loss = float(np.mean(losses))\n",
    "                return train_loss\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "        for epoch in range(config.max_epochs):\n",
    "\n",
    "            train_loss = run_epoch('train', epoch)\n",
    "            writer.add_scalar('loss/train_loss',train_loss,epoch)\n",
    "            if self.test_dataset is not None:\n",
    "                test_loss = run_epoch('test', epoch)\n",
    "                \n",
    "                x = (epoch+1) * len(self.train_dataset)# + it * self.config.batch_size\n",
    "                writer.add_scalar('loss/test_loss',test_loss,x)\n",
    "                MolFormulas = self.config.MolFormulas\n",
    "                logps = self.config.logps\n",
    "                df, completions = check_model_output(model, MolFormulas, logps, batch_size = 16)  \n",
    "                results = check_df(df)\n",
    "                for i in ['MolFormula_success','MolFormula_failure','smiles_failure_ratio']:\n",
    "                    writer.add_scalar('results/'+i,results[i],x)\n",
    "                writer.add_scalar('results_logp/'+'logp_mse',results['logp_mse'],x)\n",
    "            \n",
    "                self.save_checkpoint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 991: train loss 0.26414. lr 6.000000e-04:   0%|          | 992/712198 [02:53<35:00:14,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINS Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 13978: train loss 0.18335. lr 6.000000e-04:   2%|▏         | 13979/712198 [41:38<34:47:06,  5.58it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 21525: train loss 0.17540. lr 6.000000e-04:   3%|▎         | 21526/712198 [1:12:48<33:02:18,  5.81it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 29043: train loss 0.16652. lr 5.999999e-04:   4%|▍         | 29043/712198 [1:34:23<32:42:27,  5.80it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 36538: train loss 0.16306. lr 5.999999e-04:   5%|▌         | 36538/712198 [1:55:54<32:21:14,  5.80it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 43630: train loss 0.16244. lr 5.999999e-04:   6%|▌         | 43631/712198 [2:25:23<32:01:06,  5.80it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 51110: train loss 0.16011. lr 5.999998e-04:   7%|▋         | 51111/712198 [2:46:51<31:50:54,  5.77it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 58591: train loss 0.16273. lr 5.999998e-04:   8%|▊         | 58591/712198 [3:08:20<31:15:08,  5.81it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 66078: train loss 0.16036. lr 5.999997e-04:   9%|▉         | 66078/712198 [3:39:22<30:55:36,  5.80it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 73268: train loss 0.15922. lr 5.999996e-04:  10%|█         | 73269/712198 [4:00:01<30:38:01,  5.79it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 79999: train loss 0.15480. lr 5.999995e-04:  11%|█         | 79999/712198 [4:19:21<30:14:56,  5.81it/s]12/16/2020 03:53:04 - INFO - __main__ -   test loss: 0.163680\n",
      "12/16/2020 04:00:56 - INFO - __main__ -   saving /opt/data/train_dir/20201215_DF_gpt_smiles_full_orga_low_capacity/\n",
      "epoch 1 iter 80183: train loss 0.15925. lr 5.999995e-04:  11%|█▏        | 80184/712198 [4:28:57<30:05:26,  5.83it/s]    IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 87548: train loss 0.15948. lr 5.999994e-04:  12%|█▏        | 87548/712198 [4:50:06<29:57:59,  5.79it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 94947: train loss 0.15831. lr 5.999993e-04:  13%|█▎        | 94947/712198 [5:11:20<29:30:48,  5.81it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 101986: train loss 0.16008. lr 5.999992e-04:  14%|█▍        | 101987/712198 [5:41:03<29:12:01,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in data loader: idx = 89839496\n",
      "(89839497.0, 'Cc1cc2ccccc2c2c1op(N([C@H](C)c1ccccc1)[C@H](C)c1ccccc1)oc1c(C)cc3ccccc3c12', 567.669, 11.674000000000001, 42, 'C38H34NO2P', 74, 'False', 'True')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 102257: train loss 0.15748. lr 5.999992e-04:  14%|█▍        | 102258/712198 [5:41:50<29:08:53,  5.81it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 109582: train loss 0.15503. lr 5.999991e-04:  15%|█▌        | 109583/712198 [6:02:52<28:47:15,  5.81it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 116980: train loss 0.15671. lr 5.999990e-04:  16%|█▋        | 116980/712198 [6:24:05<28:28:28,  5.81it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 123370: train loss 0.15587. lr 5.999989e-04:  17%|█▋        | 123371/712198 [6:51:22<28:10:37,  5.80it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 124336: train loss 0.15736. lr 5.999989e-04:  17%|█▋        | 124337/712198 [6:54:08<28:02:29,  5.82it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 130735: train loss 0.15228. lr 5.999988e-04:  18%|█▊        | 130736/712198 [7:12:32<27:56:05,  5.78it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 131752: train loss 0.15806. lr 5.999987e-04:  18%|█▊        | 131753/712198 [7:15:27<27:43:17,  5.82it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 138135: train loss 0.15557. lr 5.999986e-04:  19%|█▉        | 138135/712198 [7:33:46<27:27:21,  5.81it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 139186: train loss 0.15430. lr 5.999986e-04:  20%|█▉        | 139187/712198 [7:36:47<27:29:09,  5.79it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 145541: train loss 0.14998. lr 5.999985e-04:  20%|██        | 145542/712198 [8:04:23<27:05:10,  5.81it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 146582: train loss 0.15506. lr 5.999985e-04:  21%|██        | 146582/712198 [8:07:22<27:05:16,  5.80it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 153952: train loss 0.15681. lr 5.999983e-04:  22%|██▏       | 153953/712198 [8:28:32<26:41:57,  5.81it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 159999: train loss 0.15401. lr 5.999982e-04:  22%|██▏       | 159999/712198 [8:45:53<26:21:50,  5.82it/s]12/16/2020 08:19:36 - INFO - __main__ -   test loss: 0.160711\n",
      "12/16/2020 08:28:14 - INFO - __main__ -   saving /opt/data/train_dir/20201215_DF_gpt_smiles_full_orga_low_capacity/\n",
      "epoch 1 iter 161478: train loss 0.15791. lr 5.999981e-04:  23%|██▎       | 161478/712198 [8:59:57<26:26:01,  5.79it/s]    IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 168927: train loss 0.15697. lr 5.999979e-04:  24%|██▎       | 168927/712198 [9:21:20<25:54:08,  5.83it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 176318: train loss 0.15678. lr 5.999978e-04:  25%|██▍       | 176318/712198 [9:42:33<25:35:19,  5.82it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 183733: train loss 0.15671. lr 5.999976e-04:  26%|██▌       | 183734/712198 [10:13:04<25:16:39,  5.81it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 185328: train loss 0.15453. lr 5.999975e-04:  26%|██▌       | 185329/712198 [10:17:39<25:10:53,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in data loader: idx = 90011692\n",
      "(90011693.0, 'c1cc2np[nH]c2cn1', 137.082, 1.538, 9, 'C5H4N3P', 16, 'False', 'True')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 186782: train loss 0.15299. lr 5.999975e-04:  26%|██▌       | 186783/712198 [10:21:50<25:06:56,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in data loader: idx = 88994074\n",
      "(88994075.0, 'CO[C@@]12[C@H]3[C@H]4[C@@H]5[C@@H]6[C@@H]3[C@@H]3[C@@H]7[C@H]8[C@@H]9[C@@H]%10[C@@H]([C@H]1[C@@H]4[C@@H]%10[C@@H]5[C@@]9(Br)[C@H]76)[C@@H]8[C@H]32', 369.30199999999996, 2.747, 23, 'C21H21BrO', 146, 'False', 'True')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 188986: train loss 0.15906. lr 5.999974e-04:  27%|██▋       | 188987/712198 [10:28:09<24:58:56,  5.82it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 195982: train loss 0.16102. lr 5.999972e-04:  28%|██▊       | 195982/712198 [10:48:13<24:42:47,  5.80it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 203107: train loss 0.15775. lr 5.999970e-04:  29%|██▊       | 203107/712198 [11:18:05<24:22:46,  5.80it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 210505: train loss 0.15712. lr 5.999968e-04:  30%|██▉       | 210505/712198 [11:39:18<24:01:00,  5.80it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 217730: train loss 0.15823. lr 5.999966e-04:  31%|███       | 217730/712198 [12:00:02<23:39:11,  5.81it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 225174: train loss 0.15168. lr 5.999963e-04:  32%|███▏      | 225175/712198 [12:30:10<23:15:11,  5.82it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 232286: train loss 0.15590. lr 5.999961e-04:  33%|███▎      | 232286/712198 [12:50:35<22:58:30,  5.80it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 239999: train loss 0.15477. lr 5.999958e-04:  34%|███▎      | 239999/712198 [13:12:44<22:35:46,  5.80it/s]12/16/2020 12:46:27 - INFO - __main__ -   test loss: 0.159140\n",
      "12/16/2020 12:54:34 - INFO - __main__ -   saving /opt/data/train_dir/20201215_DF_gpt_smiles_full_orga_low_capacity/\n",
      "epoch 1 iter 241359: train loss 0.15346. lr 5.999958e-04:  34%|███▍      | 241360/712198 [13:25:57<22:29:21,  5.82it/s]    IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 248812: train loss 0.15341. lr 5.999955e-04:  35%|███▍      | 248813/712198 [13:47:21<22:05:49,  5.83it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 253704: train loss 0.15370. lr 5.999954e-04:  36%|███▌      | 253704/712198 [14:01:23<22:01:58,  5.78it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# # initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=max_epochs, batch_size=batch_size, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(train_dataset)*block_size,\n",
    "                      num_workers=2, ckpt_path = experiment_path,\n",
    "                     test_every_iteration = test_every_iteration, MolFormulas = MolFormulas, logps = logps) ## 10000 * 128 = 1M approx\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "trainer.train(writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, completion = check_model_output(model,['C16H32N4O2'],[1.0],batch_size=16,temperature=2)\n",
    "df = completion_to_pandas(completion)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = 'CCOC(=O)c1ccc(O[C@H]2CCC[C@](C)(OCC)CC2)c(OCC2CCOCC2)c1'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_smiles",
   "language": "python",
   "name": "pytorch_smiles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
