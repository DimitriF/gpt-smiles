{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 170\n",
    "# max_len = 20\n",
    "batch_size = 128\n",
    "n_layer = 8 \n",
    "n_head = 4\n",
    "n_embd = 256\n",
    "max_epochs = 20\n",
    "\n",
    "test_every_iteration = 10000\n",
    "\n",
    "stop_token = '$'\n",
    "split_token = '&'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/14/2020 15:11:01 - INFO - rdkit -   Enabling RDKit 2020.09.1 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "\n",
    "# from flattenV3Advanced import flattenData\n",
    "\n",
    "# make deterministic\n",
    "from minGPT.mingpt.utils import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = torch.cuda.current_device()\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "from minGPT.mingpt.model import GPT, GPTConfig\n",
    "# from minGPT.mingpt.trainer import Trainer, TrainerConfig\n",
    "# from minGPT.mingpt.utils import sample\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from rdkit.Chem.rdMolDescriptors import CalcMolFormula\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "from rdkit import Chem\n",
    "from rdkit.rdBase import BlockLogs\n",
    "from rdkit.Chem import PandasTools\n",
    "block = BlockLogs() ## not sure we want to block all but rdkit complain when wrong smiles are sent...\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "## dataset path\n",
    "data_path = '/home/teleport/perso/smiles.db'\n",
    "EXPERIMENT = '20201214_DF_gpt_smiles_full_orga_low_capacity'\n",
    "experiment_path = \"/opt/data/train_dir/\" + EXPERIMENT + \"/\"\n",
    "if not os.path.exists(experiment_path):\n",
    "    os.mkdir(experiment_path)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trains and tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINS Task: overwriting (reusing) task id=598dc957bc7545469db6e176e112055a\n",
      "TRAINS results page: http://172.22.4.157:8080/projects/8f2ac9400a46477683280746b293e09f/experiments/598dc957bc7545469db6e176e112055a/output/log\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "writer = SummaryWriter(experiment_path + '/' + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\")\n",
    "from trains import Task\n",
    "import trains\n",
    "task = Task.init(project_name='gpt_smiles', task_name=EXPERIMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created and Successfully Connected to SQLite\n"
     ]
    }
   ],
   "source": [
    "sqliteConnection = sqlite3.connect('/home/teleport/perso/smiles.db')\n",
    "cursor = sqliteConnection.cursor()\n",
    "print(\"Database created and Successfully Connected to SQLite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-14 15:11:02,080 - trains.Repository Detection - WARNING - Can't get branch information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-14 15:11:02,090 - trains.Repository Detection - WARNING - Can't get commit information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-14 15:11:02,100 - trains.Repository Detection - WARNING - Can't get root information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-14 15:11:02,110 - trains.Repository Detection - WARNING - Can't get status information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-14 15:11:02,128 - trains.Repository Detection - WARNING - Can't get diff information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-14 15:11:02,137 - trains.Repository Detection - WARNING - Can't get modified information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(90943000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('SELECT COUNT() FROM smiles_train')\n",
    "n_train = cursor.fetchone()\n",
    "n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(457000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('SELECT COUNT() FROM smiles_test')\n",
    "n_test = cursor.fetchone()\n",
    "n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINS new version available: upgrade to v0.16.4 is recommended!\n"
     ]
    }
   ],
   "source": [
    "cursor.execute('SELECT * FROM smiles_test')\n",
    "dataset_test = cursor.fetchall()\n",
    "dataset_test = pd.DataFrame(dataset_test, columns = ['id','smiles','mwt','logp','n_atoms','MolFormula','l','CHNO','orga'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the logps and mol formulas for testing\n",
    "MolFormulas = list(dataset_test['MolFormula'].unique())\n",
    "MolFormulas = random.sample(MolFormulas, 500)\n",
    "logps = [list(dataset_test[dataset_test['MolFormula'] == i]['logp'])[0] for i in MolFormulas]\n",
    "# logps = [list(i['logp'])[0] for i in logps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (sqliteConnection):\n",
    "    sqliteConnection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['C','c','N','n','O','o','H','B','r','l','I','F','S','s','P','.','[',']','(',')','-','+','=','@','/','\\\\','#','$','&']\n",
    "word_list += list(string.digits)\n",
    "stoi = { ch:i for i,ch in enumerate(word_list) }\n",
    "itos = { i:ch for i,ch in enumerate(word_list) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for checking the smiles and tokenize them, detokenize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_idx(data, idx, stoi, block_size = None, stop_token = '$', split_token = '&'):\n",
    "    return tokenize_smiles(data['smiles'][idx],data['MolFormula'][idx],data['logp'][idx], stoi, block_size=block_size, stop_token = '$', split_token = '&')\n",
    "\n",
    "def tokenize_smiles(smiles, molecular_formula, logp, stoi, block_size = None, stop_token = '$', split_token = '&'):\n",
    "    chunk = str(round(logp,1)) + split_token + molecular_formula + split_token + smiles + stop_token\n",
    "    if block_size is not None:\n",
    "        chunk = chunk + stop_token*(block_size - len(chunk))\n",
    "    data = [stoi[s] for s in chunk]\n",
    "    return data\n",
    "\n",
    "def y_to_completion(y, itos):\n",
    "    return ''.join([itos[int(i)] for i in y])\n",
    "    \n",
    "\n",
    "# def check_completion(completion, stop_token = '$', split_token = '&'):\n",
    "#     '''\n",
    "#     return a string with the status of the check:\n",
    "#     full_failure: cannot process it at all\n",
    "#     smiles_failure: MolFromSmiles return None\n",
    "#     MolFormula_failure: CalcMolFormula from the smile is different\n",
    "#     success: CalcMolFormula from the smile is same\n",
    "#     '''\n",
    "    \n",
    "#     try:\n",
    "#         completion = completion.split(split_token)\n",
    "#         logp = completion[0]\n",
    "#         MolFormula = completion[1]\n",
    "#         smiles = completion[2].split(stop_token)[0]\n",
    "#         mol = Chem.MolFromSmiles(smiles)\n",
    "#         if mol is None:\n",
    "#             return 'smiles_failure'\n",
    "#         MolFormula_smiles = CalcMolFormula(mol)\n",
    "#         logp_smiles = Descriptors.MolLogP(mol)\n",
    "#         if MolFormula != MolFormula_smiles:\n",
    "#             return 'MolFormula_failure'\n",
    "#         return 'success'\n",
    "#     except:\n",
    "#         return 'full_failure'\n",
    "\n",
    "    \n",
    "def check_model_output(model, MolFormulas, logps, stop_token = '$', split_token = '&', batch_size = 4,temperature=0.9):\n",
    "    completions = []\n",
    "    for i, j  in zip(MolFormulas, logps):        \n",
    "        data = [stoi[s] for s in str(round(j,1)) + split_token + i + split_token]\n",
    "        x = [torch.tensor(data, dtype=torch.long).unsqueeze(0).to(device)]\n",
    "        y = sample_clean(model, x, steps = block_size, temperature=temperature, sample=True, top_k=5, itos = itos, stop_token = '$', batch_size = batch_size)\n",
    "        for idx in range(y.size(0)):    \n",
    "            completion = y_to_completion(y[idx], itos)\n",
    "            completions.append(completion)\n",
    "    df = completion_to_pandas(completions)\n",
    "            \n",
    "    return df, completions\n",
    "    \n",
    "def completion_to_pandas(completion):\n",
    "    smiles = []\n",
    "    MolFormulaInput = []\n",
    "    MolFormulaOutput = []\n",
    "    logpInput = []\n",
    "    logpOutput = []\n",
    "    for i in completion:\n",
    "#         print(i)\n",
    "        i = i.split(split_token)\n",
    "        logpInput.append(i[0])\n",
    "        MolFormulaInput.append(i[1])\n",
    "        smile = i[2].split(stop_token)[0]\n",
    "        smiles.append(smile)\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        if mol is None:\n",
    "            MolFormulaOutput.append('fail')\n",
    "            logpOutput.append('fail')\n",
    "        else:\n",
    "            MolFormulaOutput.append(CalcMolFormula(mol))\n",
    "            logpOutput.append(Descriptors.MolLogP(mol))\n",
    "    df = pd.DataFrame(list(zip(logpInput,MolFormulaInput, smiles,logpOutput, MolFormulaOutput)), \n",
    "               columns =['logpInput','MolFormulaInput', 'smiles', 'logpOutput', 'MolFormulaOutput']) \n",
    "    PandasTools.AddMoleculeColumnToFrame(df,smilesCol='smiles')\n",
    "    return(df)\n",
    "\n",
    "def check_df(df):\n",
    "    df2 = df[df['MolFormulaOutput'] != 'fail']\n",
    "    smiles_failure_ratio = (len(df) - len(df2))/len(df)\n",
    "    MolFormula_success = sum(df2['MolFormulaInput'] == df2['MolFormulaOutput'])/len(df)\n",
    "    MolFormula_failure = sum(df2['MolFormulaInput'] != df2['MolFormulaOutput'])/len(df)\n",
    "    logp_mse = mean_squared_error(df2['logpInput'],df2['logpOutput'])\n",
    "    return {'smiles_failure_ratio':smiles_failure_ratio, \n",
    "            'MolFormula_failure':MolFormula_failure,\n",
    "           'MolFormula_success':MolFormula_success,\n",
    "           'logp_mse':logp_mse}\n",
    "\n",
    "\n",
    "def add_mol(writer, smiles, global_step=None, size=(300, 300)):\n",
    "    \"\"\"\n",
    "    Adds a molecule to the images section of Tensorboard.\n",
    "    \"\"\"\n",
    "    img_transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    image = Chem.Draw.MolToImage(mol, size=size)\n",
    "    writer.add_image(smiles, img_transform(image), global_step) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = tokenize_idx(data_test, 0, stoi)\n",
    "# completion = y_to_completion(y, itos)\n",
    "# completion\n",
    "\n",
    "# check_df(completion_to_pandas([completion]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smiles_failure_ratio': 0.0,\n",
       " 'MolFormula_failure': 0.0,\n",
       " 'MolFormula_success': 1.0,\n",
       " 'logp_mse': 0.26863489000000185}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = '1.2&C23H39N5O&CN(C)c1ccc(CNC(=O)CN2CCC(C)(CN3CCN(C)CC3)CC2)cc1'\n",
    "check_df(completion_to_pandas([completion]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([32, 15, 29, 28,  0, 30, 36,  6, 31, 33,  0,  9,  2, 32,  4, 31, 12, 28,\n",
       "          0,  4,  1, 30,  1,  1, 18,  0,  2, 31,  0,  0,  2, 18,  0, 32, 22,  2,\n",
       "          0, 16,  0, 23,  6, 17, 18,  0, 19, 12, 32, 19,  0,  0, 31, 19,  1,  1,\n",
       "         18,  0,  9, 19,  1, 30,  4,  0, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27]),\n",
       " tensor([15, 29, 28,  0, 30, 36,  6, 31, 33,  0,  9,  2, 32,  4, 31, 12, 28,  0,\n",
       "          4,  1, 30,  1,  1, 18,  0,  2, 31,  0,  0,  2, 18,  0, 32, 22,  2,  0,\n",
       "         16,  0, 23,  6, 17, 18,  0, 19, 12, 32, 19,  0,  0, 31, 19,  1,  1, 18,\n",
       "          0,  9, 19,  1, 30,  4,  0, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SmilesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, table, n, id_string, block_size, stoi, itos):\n",
    "        \n",
    "        self.table = table\n",
    "        self.n = n\n",
    "        self.id_string = id_string\n",
    "        self.stoi = stoi\n",
    "        self.itos = itos\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = len(self.stoi)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx,verbose=False):\n",
    "        try:\n",
    "            sqliteConnection = sqlite3.connect('/home/teleport/perso/smiles.db')\n",
    "            cursor = sqliteConnection.cursor()\n",
    "            sqlite_select_Query = \"SELECT * FROM \" + self.table + \"  WHERE \" + self.id_string + \" = \" + str(idx+1)\n",
    "            cursor.execute(sqlite_select_Query)\n",
    "            record = cursor.fetchone()\n",
    "            if (sqliteConnection):\n",
    "                sqliteConnection.close()\n",
    "#             print(record[5])\n",
    "            smiles = record[1]\n",
    "            molecular_formula = record[5]\n",
    "            logp = record[3]\n",
    "            converted_chunk = tokenize_smiles(smiles, molecular_formula, logp, self.stoi, self.block_size)\n",
    "        except:\n",
    "            print('error in data loader: idx = ' + str(idx))\n",
    "            return self.__getitem__(random.randint(0, self.n))\n",
    "        x = torch.tensor(converted_chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(converted_chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "            \n",
    "train_dataset = SmilesDataset('smiles_train', n_train[0], 'id', block_size, stoi, itos) \n",
    "test_dataset = SmilesDataset('smiles_test', int(n_test[0]/4), 'id', block_size, stoi, itos) \n",
    "\n",
    "df = test_dataset.__getitem__(idx = 10000,verbose=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 'COc1ccc(C(=O)N2CCN(C[C@H]3CC3(Cl)Cl)CC2)cc1OC', 373.28, 2.655, 24, 'C17H22Cl2N2O3', 45, 'False', 'True')\n"
     ]
    }
   ],
   "source": [
    "sqliteConnection = sqlite3.connect('/home/teleport/perso/smiles.db')\n",
    "cursor = sqliteConnection.cursor()\n",
    "sqlite_select_Query = \"SELECT * FROM \" + 'smiles_test' + \"  WHERE \" + 'id' + \" = \" + str(1)\n",
    "cursor.execute(sqlite_select_Query)\n",
    "record = cursor.fetchone()\n",
    "print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/14/2020 15:11:13 - INFO - minGPT.mingpt.model -   number of parameters: 6.382080e+06\n"
     ]
    }
   ],
   "source": [
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=n_layer, n_head=n_head, n_embd=n_embd)\n",
    "model = GPT(mconf)\n",
    "# model.load_state_dict(torch.load(experiment_path + 'model_0.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[:, [-1]]] = -float('Inf')\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_clean(model, xs, steps, temperature=1.0, sample=False, top_k=None, itos = None, stop_token = '$',batch_size = 4):\n",
    "#     block_size = model.get_block_size()\n",
    "    model.eval()\n",
    "    xs = sorted(xs, key = lambda x: x.size(1))\n",
    "    ## add the batch size\n",
    "    xs = [torch.cat(batch_size*[x]) for x in xs]\n",
    "    \n",
    "    ## start by making them the same length\n",
    "    x = xs[0]\n",
    "    for i in range(1,len(xs)): # nothing to do if length 1\n",
    "        while x.size(1) < xs[i].size(1):\n",
    "            x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
    "            logits, _ = model(x_cond)\n",
    "            # pluck the logits at the final step and scale by temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop probabilities to only the top k options\n",
    "            if top_k is not None:\n",
    "                logits = top_k_logits(logits, top_k)\n",
    "            # apply softmax to convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution or take the most likely\n",
    "            if sample:\n",
    "                ix = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "            # append to the sequence and continue\n",
    "            x = torch.cat((x, ix), dim=1)\n",
    "        x= torch.cat((x,xs[i]), dim=0)\n",
    "    \n",
    "    \n",
    "    ## now we can keep going...\n",
    "    keep_going = True\n",
    "    while keep_going:\n",
    "#         print(x.size(1))\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
    "        logits, _ = model(x_cond)\n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "        # append to the sequence and continue\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "        if x.size(1) == block_size:\n",
    "            keep_going = False\n",
    "        if itos is not None:\n",
    "            sequences_over = 0\n",
    "            for i in range(x.size(0)):                \n",
    "                if itos[int(ix[i])] == stop_token:\n",
    "                    sequences_over += 1\n",
    "            if sequences_over == x.size(0):\n",
    "                keep_going = False\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    batch_size = 64\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False\n",
    "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = None\n",
    "    num_workers = 0 # for DataLoader\n",
    "    test_every_iteration = 50000\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_dataset, test_dataset, config):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.config = config\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        logger.info(\"saving %s\", self.config.ckpt_path)\n",
    "        torch.save(raw_model.state_dict(), self.config.ckpt_path + 'model.pt')\n",
    "\n",
    "    def train(self, writer):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "\n",
    "        def run_epoch(split, epoch):\n",
    "            is_train = split == 'train'\n",
    "            model.train(is_train)\n",
    "            data = self.train_dataset if is_train else self.test_dataset\n",
    "            loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
    "                                batch_size=config.batch_size,\n",
    "                                num_workers=config.num_workers)\n",
    "\n",
    "            losses = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "            for it, (x, y) in pbar:\n",
    "\n",
    "                # place data on the correct device\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                # forward the model\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    logits, loss = model(x, y)\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "\n",
    "                if is_train:\n",
    "\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # decay the learning rate based on our progress\n",
    "                    if config.lr_decay:\n",
    "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "                        if self.tokens < config.warmup_tokens:\n",
    "                            # linear warmup\n",
    "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                        else:\n",
    "                            # cosine learning rate decay\n",
    "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                        lr = config.learning_rate * lr_mult\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                    else:\n",
    "                        lr = config.learning_rate\n",
    "\n",
    "                    # report progress\n",
    "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
    "                    \n",
    "                    ## check if iterations is modulo of iteration interval and performa  test:\n",
    "                    \n",
    "                    if (it+1) % config.test_every_iteration == 0 and self.test_dataset is not None:\n",
    "                        test_loss = run_epoch('test', epoch)\n",
    "                        x = (epoch) * len(self.train_dataset) + it * self.config.batch_size\n",
    "                        writer.add_scalar('loss/test_loss',test_loss,x)\n",
    "                        \n",
    "                        MolFormulas = self.config.MolFormulas\n",
    "                        logps = self.config.logps\n",
    "                        df, completions = check_model_output(model, MolFormulas, logps, batch_size = 16)  \n",
    "                        results = check_df(df)\n",
    "                        for i in ['MolFormula_success','MolFormula_failure','smiles_failure_ratio']:\n",
    "                            writer.add_scalar('results/'+i,results[i],x)\n",
    "                        writer.add_scalar('results_logp/'+'logp_mse',results['logp_mse'],x)\n",
    "                    \n",
    "\n",
    "            if not is_train:\n",
    "                test_loss = float(np.mean(losses))\n",
    "                logger.info(\"test loss: %f\", test_loss)\n",
    "                return test_loss\n",
    "            else:\n",
    "                train_loss = float(np.mean(losses))\n",
    "                return train_loss\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "        for epoch in range(config.max_epochs):\n",
    "\n",
    "            train_loss = run_epoch('train', epoch)\n",
    "            writer.add_scalar('loss/train_loss',train_loss,epoch)\n",
    "            if self.test_dataset is not None:\n",
    "                test_loss = run_epoch('test', epoch)\n",
    "                \n",
    "                x = (epoch+1) * len(self.train_dataset)# + it * self.config.batch_size\n",
    "                writer.add_scalar('loss/test_loss',test_loss,x)\n",
    "                MolFormulas = self.config.MolFormulas\n",
    "                logps = self.config.logps\n",
    "                df, completions = check_model_output(model, MolFormulas, logps, batch_size = 16)  \n",
    "                results = check_df(df)\n",
    "                for i in ['MolFormula_success','MolFormula_failure','smiles_failure_ratio']:\n",
    "                    writer.add_scalar('results/'+i,results[i],x)\n",
    "                writer.add_scalar('results_logp/'+'logp_mse',results['logp_mse'],x)\n",
    "\n",
    "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
    "            good_model = self.test_dataset is None or test_loss < best_loss\n",
    "            if self.config.ckpt_path is not None and good_model:\n",
    "                best_loss = test_loss\n",
    "                self.save_checkpoint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 912: train loss 0.21718. lr 6.000000e-04:   0%|          | 913/710493 [02:41<35:11:32,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINS Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 9999: train loss 0.13617. lr 6.000000e-04:   1%|▏         | 9999/710493 [29:45<34:47:27,  5.59it/s]12/14/2020 15:41:59 - INFO - __main__ -   test loss: 0.131248\n",
      "epoch 1 iter 12558: train loss 0.12393. lr 6.000000e-04:   2%|▏         | 12559/710493 [40:38<33:20:37,  5.81it/s]   IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 19345: train loss 0.11474. lr 6.000000e-04:   3%|▎         | 19345/710493 [1:00:05<33:02:33,  5.81it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 25978: train loss 0.10568. lr 6.000000e-04:   4%|▎         | 25979/710493 [1:22:39<32:42:51,  5.81it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 39999: train loss 0.09835. lr 5.999999e-04:   6%|▌         | 39999/710493 [2:06:26<31:56:55,  5.83it/s]12/14/2020 17:18:40 - INFO - __main__ -   test loss: 0.099650\n",
      "epoch 1 iter 42744: train loss 0.09971. lr 5.999999e-04:   6%|▌         | 42745/710493 [2:17:49<31:46:19,  5.84it/s]   IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 49999: train loss 0.09961. lr 5.999998e-04:   7%|▋         | 49999/710493 [2:38:36<31:35:59,  5.81it/s]12/14/2020 17:50:50 - INFO - __main__ -   test loss: 0.096367\n",
      "epoch 1 iter 52526: train loss 0.09426. lr 5.999998e-04:   7%|▋         | 52527/710493 [2:49:23<31:32:12,  5.80it/s]   IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 59540: train loss 0.09042. lr 5.999997e-04:   8%|▊         | 59540/710493 [3:09:29<31:05:40,  5.82it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "12/14/2020 18:23:02 - INFO - __main__ -   test loss: 0.093602\n",
      "epoch 1 iter 62715: train loss 0.09187. lr 5.999997e-04:   9%|▉         | 62716/710493 [3:22:10<30:52:26,  5.83it/s]   IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 69999: train loss 0.09333. lr 5.999996e-04:  10%|▉         | 69999/710493 [3:43:02<30:35:21,  5.82it/s]12/14/2020 18:55:16 - INFO - __main__ -   test loss: 0.092639\n",
      "epoch 1 iter 72615: train loss 0.09179. lr 5.999996e-04:  10%|█         | 72616/710493 [3:54:05<30:25:51,  5.82it/s]   IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 79972: train loss 0.09230. lr 5.999995e-04:  11%|█▏        | 79972/710493 [4:15:10<30:08:33,  5.81it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 87187: train loss 0.09403. lr 5.999994e-04:  12%|█▏        | 87187/710493 [4:39:19<29:45:58,  5.82it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 94622: train loss 0.08896. lr 5.999994e-04:  13%|█▎        | 94623/710493 [5:04:07<29:22:40,  5.82it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 101769: train loss 0.09105. lr 5.999992e-04:  14%|█▍        | 101770/710493 [5:28:08<29:04:43,  5.81it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 108930: train loss 0.08826. lr 5.999991e-04:  15%|█▌        | 108931/710493 [5:48:39<28:47:10,  5.80it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "epoch 1 iter 118623: train loss 0.09051. lr 5.999990e-04:  17%|█▋        | 118623/710493 [6:19:54<28:14:22,  5.82it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# # initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=max_epochs, batch_size=batch_size, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(train_dataset)*block_size,\n",
    "                      num_workers=2, ckpt_path = experiment_path,\n",
    "                     test_every_iteration = test_every_iteration, MolFormulas = MolFormulas, logps = logps) ## 10000 * 128 = 1M approx\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "trainer.train(writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, completion = check_model_output(model,['C16H32N4O2'],[1.0],batch_size=16,temperature=2)\n",
    "df = completion_to_pandas(completion)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = 'CCOC(=O)c1ccc(O[C@H]2CCC[C@](C)(OCC)CC2)c(OCC2CCOCC2)c1'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_smiles",
   "language": "python",
   "name": "pytorch_smiles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
