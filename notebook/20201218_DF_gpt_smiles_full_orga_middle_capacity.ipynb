{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 170\n",
    "# max_len = 20\n",
    "batch_size = 128\n",
    "n_layer = 8 \n",
    "n_head = 8\n",
    "n_embd = 256\n",
    "max_epochs = 1\n",
    "\n",
    "test_every_iteration = 20000\n",
    "\n",
    "stop_token = '$'\n",
    "split_token = '&'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/18/2020 14:06:42 - INFO - rdkit -   Enabling RDKit 2020.09.1 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "\n",
    "# from flattenV3Advanced import flattenData\n",
    "\n",
    "# make deterministic\n",
    "from minGPT.mingpt.utils import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = torch.cuda.current_device()\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "from minGPT.mingpt.model import GPT, GPTConfig\n",
    "# from minGPT.mingpt.trainer import Trainer, TrainerConfig\n",
    "# from minGPT.mingpt.utils import sample\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from rdkit.Chem.rdMolDescriptors import CalcMolFormula\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "from rdkit import Chem\n",
    "from rdkit.rdBase import BlockLogs\n",
    "from rdkit.Chem import PandasTools\n",
    "block = BlockLogs() ## not sure we want to block all but rdkit complain when wrong smiles are sent...\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "## dataset path\n",
    "data_path = '/home/teleport/perso/smiles_clean.db'\n",
    "EXPERIMENT = '20201218_DF_gpt_smiles_full_orga_middle_capacity'\n",
    "experiment_path = \"/opt/data/train_dir/\" + EXPERIMENT + \"/\"\n",
    "if not os.path.exists(experiment_path):\n",
    "    os.mkdir(experiment_path)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trains and tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINS Task: created new task id=746bc0e084de4ae2bbb99f774888245e\n",
      "TRAINS results page: http://172.22.4.157:8080/projects/8f2ac9400a46477683280746b293e09f/experiments/746bc0e084de4ae2bbb99f774888245e/output/log\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "writer = SummaryWriter(experiment_path + '/' + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\")\n",
    "from trains import Task\n",
    "import trains\n",
    "task = Task.init(project_name='gpt_smiles', task_name=EXPERIMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created and Successfully Connected to SQLite\n"
     ]
    }
   ],
   "source": [
    "sqliteConnection = sqlite3.connect(data_path)\n",
    "cursor = sqliteConnection.cursor()\n",
    "print(\"Database created and Successfully Connected to SQLite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-18 14:06:42,930 - trains.Repository Detection - WARNING - Can't get branch information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-18 14:06:42,943 - trains.Repository Detection - WARNING - Can't get commit information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-18 14:06:42,955 - trains.Repository Detection - WARNING - Can't get root information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-18 14:06:42,966 - trains.Repository Detection - WARNING - Can't get status information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-18 14:06:42,985 - trains.Repository Detection - WARNING - Can't get diff information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n",
      "2020-12-18 14:06:42,996 - trains.Repository Detection - WARNING - Can't get modified information for git repo in /home/teleport/.conda/envs/pytorch_smiles/lib/python3.8/site-packages\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(91161254,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('SELECT COUNT() FROM smiles_train')\n",
    "n_train = cursor.fetchone()\n",
    "n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162554,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('SELECT COUNT() FROM smiles_test')\n",
    "n_test = cursor.fetchone()\n",
    "n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolFormula</th>\n",
       "      <th>train</th>\n",
       "      <th>logp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C20H24FN3O2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C18H17NO5S</td>\n",
       "      <td>0</td>\n",
       "      <td>2.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C15H18F3NO4S</td>\n",
       "      <td>0</td>\n",
       "      <td>2.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C15H15F3N4O3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C16H13ClN6O2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>C31H33F2N3O6S</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>C15H21N9O5</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>C12H5F4IN2O</td>\n",
       "      <td>0</td>\n",
       "      <td>3.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>C12H13FINOS2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>C12H6BrF4N5O</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1139 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         MolFormula  train   logp\n",
       "0       C20H24FN3O2      0  2.800\n",
       "1        C18H17NO5S      0  2.994\n",
       "2      C15H18F3NO4S      0  2.860\n",
       "3      C15H15F3N4O3      0  2.748\n",
       "4      C16H13ClN6O2      0  2.603\n",
       "...             ...    ...    ...\n",
       "1134  C31H33F2N3O6S      0  1.000\n",
       "1135     C15H21N9O5      0 -1.537\n",
       "1136    C12H5F4IN2O      0  3.495\n",
       "1137   C12H13FINOS2      0  1.000\n",
       "1138   C12H6BrF4N5O      0  1.000\n",
       "\n",
       "[1139 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('SELECT *  FROM MolFormulas WHERE train = 0')\n",
    "MolFormulas_test = cursor.fetchall()\n",
    "MolFormulas_test = pd.DataFrame(MolFormulas_test, columns = ['MolFormula','train','logp'])\n",
    "MolFormulas_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the logps and mol formulas for testing\n",
    "MolFormulas = list(MolFormulas_test['MolFormula'])\n",
    "logps = list(MolFormulas_test['logp'])\n",
    "# logps = [list(i['logp'])[0] for i in logps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (sqliteConnection):\n",
    "    sqliteConnection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['C','c','N','n','O','o','H','B','r','l','I','F','S','s','P','.','[',']','(',')','-','+','=','@','/','\\\\','#','$','&']\n",
    "word_list += list(string.digits)\n",
    "stoi = { ch:i for i,ch in enumerate(word_list) }\n",
    "itos = { i:ch for i,ch in enumerate(word_list) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for checking the smiles and tokenize them, detokenize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('asdpajsdpsd'[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_idx(data, idx, stoi, block_size = None, stop_token = '$', split_token = '&'):\n",
    "#     return tokenize_smiles(data['smiles'][idx],data['MolFormula'][idx],data['logp'][idx], stoi, block_size=block_size, stop_token = '$', split_token = '&')\n",
    "\n",
    "def tokenize_smiles(smiles, molecular_formula, logp, stoi, block_size = None, stop_token = '$', split_token = '&'):\n",
    "    chunk = str(round(logp,1)) + split_token + molecular_formula + split_token + smiles + stop_token\n",
    "    if block_size is not None:\n",
    "        if len(chunk) < block_size:\n",
    "            chunk = chunk + stop_token*(block_size - len(chunk))\n",
    "        elif len(chunk) > block_size:\n",
    "            chunk = chunk[:block_size]\n",
    "    data = [stoi[s] for s in chunk]\n",
    "    return data\n",
    "\n",
    "def y_to_completion(y, itos):\n",
    "    return ''.join([itos[int(i)] for i in y])\n",
    "    \n",
    "\n",
    "# def check_completion(completion, stop_token = '$', split_token = '&'):\n",
    "#     '''\n",
    "#     return a string with the status of the check:\n",
    "#     full_failure: cannot process it at all\n",
    "#     smiles_failure: MolFromSmiles return None\n",
    "#     MolFormula_failure: CalcMolFormula from the smile is different\n",
    "#     success: CalcMolFormula from the smile is same\n",
    "#     '''\n",
    "    \n",
    "#     try:\n",
    "#         completion = completion.split(split_token)\n",
    "#         logp = completion[0]\n",
    "#         MolFormula = completion[1]\n",
    "#         smiles = completion[2].split(stop_token)[0]\n",
    "#         mol = Chem.MolFromSmiles(smiles)\n",
    "#         if mol is None:\n",
    "#             return 'smiles_failure'\n",
    "#         MolFormula_smiles = CalcMolFormula(mol)\n",
    "#         logp_smiles = Descriptors.MolLogP(mol)\n",
    "#         if MolFormula != MolFormula_smiles:\n",
    "#             return 'MolFormula_failure'\n",
    "#         return 'success'\n",
    "#     except:\n",
    "#         return 'full_failure'\n",
    "\n",
    "    \n",
    "def check_model_output(model, MolFormulas, logps, stop_token = '$', split_token = '&', batch_size = 4,temperature=0.9):\n",
    "    completions = []\n",
    "    for i, j  in zip(MolFormulas, logps):        \n",
    "        data = [stoi[s] for s in str(round(j,1)) + split_token + i + split_token]\n",
    "        x = [torch.tensor(data, dtype=torch.long).unsqueeze(0).to(device)]\n",
    "        y = sample_clean(model, x, steps = block_size, temperature=temperature, sample=True, top_k=5, itos = itos, stop_token = '$', batch_size = batch_size)\n",
    "        for idx in range(y.size(0)):    \n",
    "            completion = y_to_completion(y[idx], itos)\n",
    "            completions.append(completion)\n",
    "    df = completion_to_pandas(completions)\n",
    "            \n",
    "    return df, completions\n",
    "    \n",
    "def completion_to_pandas(completion):\n",
    "    smiles = []\n",
    "    MolFormulaInput = []\n",
    "    MolFormulaOutput = []\n",
    "    logpInput = []\n",
    "    logpOutput = []\n",
    "    for i in completion:\n",
    "#         print(i)\n",
    "        i = i.split(split_token)\n",
    "        logpInput.append(i[0])\n",
    "        MolFormulaInput.append(i[1])\n",
    "        smile = i[2].split(stop_token)[0]\n",
    "        smiles.append(smile)\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        if mol is None:\n",
    "            MolFormulaOutput.append('fail')\n",
    "            logpOutput.append('fail')\n",
    "        else:\n",
    "            MolFormulaOutput.append(CalcMolFormula(mol))\n",
    "            logpOutput.append(Descriptors.MolLogP(mol))\n",
    "    df = pd.DataFrame(list(zip(logpInput,MolFormulaInput, smiles,logpOutput, MolFormulaOutput)), \n",
    "               columns =['logpInput','MolFormulaInput', 'smiles', 'logpOutput', 'MolFormulaOutput']) \n",
    "    PandasTools.AddMoleculeColumnToFrame(df,smilesCol='smiles')\n",
    "    return(df)\n",
    "\n",
    "def check_df(df):\n",
    "    df2 = df[df['MolFormulaOutput'] != 'fail']\n",
    "    smiles_failure_ratio = (len(df) - len(df2))/len(df)\n",
    "    MolFormula_success = sum(df2['MolFormulaInput'] == df2['MolFormulaOutput'])/len(df)\n",
    "    MolFormula_failure = sum(df2['MolFormulaInput'] != df2['MolFormulaOutput'])/len(df)\n",
    "    logp_mse = mean_squared_error(df2['logpInput'],df2['logpOutput'])\n",
    "    return {'smiles_failure_ratio':smiles_failure_ratio, \n",
    "            'MolFormula_failure':MolFormula_failure,\n",
    "           'MolFormula_success':MolFormula_success,\n",
    "           'logp_mse':logp_mse}\n",
    "\n",
    "\n",
    "def add_mol(writer, smiles, global_step=None, size=(300, 300)):\n",
    "    \"\"\"\n",
    "    Adds a molecule to the images section of Tensorboard.\n",
    "    \"\"\"\n",
    "    img_transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    image = Chem.Draw.MolToImage(mol, size=size)\n",
    "    writer.add_image(smiles, img_transform(image), global_step) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = tokenize_idx(data_test, 0, stoi)\n",
    "# completion = y_to_completion(y, itos)\n",
    "# completion\n",
    "\n",
    "# check_df(completion_to_pandas([completion]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smiles_failure_ratio': 0.0,\n",
       " 'MolFormula_failure': 0.0,\n",
       " 'MolFormula_success': 1.0,\n",
       " 'logp_mse': 0.26863489000000185}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = '1.2&C23H39N5O&CN(C)c1ccc(CNC(=O)CN2CCC(C)(CN3CCN(C)CC3)CC2)cc1'\n",
    "check_df(completion_to_pandas([completion]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([32, 15, 33, 28,  0, 31, 31,  6, 32, 32,  2,  4, 33, 28,  0,  0, 18,  0,\n",
       "         19, 18,  0, 19,  1, 30,  1,  1,  1,  1,  1, 30,  4,  0,  0, 18, 22,  4,\n",
       "         19,  2, 30,  0,  0,  0, 18,  4,  0, 16,  0, 23, 23,  6, 17, 31,  0,  0,\n",
       "          4,  0, 31, 19,  0,  0, 30, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27]),\n",
       " tensor([15, 33, 28,  0, 31, 31,  6, 32, 32,  2,  4, 33, 28,  0,  0, 18,  0, 19,\n",
       "         18,  0, 19,  1, 30,  1,  1,  1,  1,  1, 30,  4,  0,  0, 18, 22,  4, 19,\n",
       "          2, 30,  0,  0,  0, 18,  4,  0, 16,  0, 23, 23,  6, 17, 31,  0,  0,  4,\n",
       "          0, 31, 19,  0,  0, 30, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 27]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SmilesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, table, n, id_string, block_size, stoi, itos):\n",
    "        \n",
    "        self.table = table\n",
    "        self.n = n\n",
    "        self.id_string = id_string\n",
    "        self.stoi = stoi\n",
    "        self.itos = itos\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = len(self.stoi)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx,verbose=False):\n",
    "        try:\n",
    "            sqliteConnection = sqlite3.connect(data_path)\n",
    "            cursor = sqliteConnection.cursor()\n",
    "            sqlite_select_Query = \"SELECT * FROM \" + self.table + \"  WHERE \" + self.id_string + \" = \" + str(idx+1)\n",
    "            cursor.execute(sqlite_select_Query)\n",
    "            record = cursor.fetchone()\n",
    "            if (sqliteConnection):\n",
    "                sqliteConnection.close()\n",
    "#             print(record[5])\n",
    "            smiles = record[1]\n",
    "            molecular_formula = record[5]\n",
    "            logp = record[3]\n",
    "            converted_chunk = tokenize_smiles(smiles, molecular_formula, logp, self.stoi, self.block_size)\n",
    "        except:\n",
    "            print('error in data loader: idx = ' + str(idx))\n",
    "            try:\n",
    "                print(record)\n",
    "            except:\n",
    "                print('could not even print record')\n",
    "            return self.__getitem__(random.randint(0, self.n))\n",
    "        x = torch.tensor(converted_chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(converted_chunk[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "            \n",
    "train_dataset = SmilesDataset('smiles_train', n_train[0], 'id', block_size, stoi, itos) \n",
    "test_dataset = SmilesDataset('smiles_test', n_test[0], 'id', block_size, stoi, itos) \n",
    "\n",
    "df = train_dataset.__getitem__(idx = 91028281,verbose=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqliteConnection = sqlite3.connect(data_path)\n",
    "# cursor = sqliteConnection.cursor()\n",
    "# sqlite_select_Query = \"SELECT * FROM \" + 'smiles_train' + \"  WHERE \" + 'id' + \" = \" + str(61678207+1)\n",
    "# cursor.execute(sqlite_select_Query)\n",
    "# record = cursor.fetchone()\n",
    "# print(record)\n",
    "# if (sqliteConnection):\n",
    "#     sqliteConnection.close()\n",
    "# #             print(record[5])\n",
    "# smiles = record[1]\n",
    "# molecular_formula = record[5]\n",
    "# logp = record[3]\n",
    "# # converted_chunk = tokenize_smiles(smiles, molecular_formula, logp, self.stoi, self.block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqliteConnection = sqlite3.connect('/home/teleport/perso/smiles.db')\n",
    "# cursor = sqliteConnection.cursor()\n",
    "# sqlite_select_Query = \"SELECT * FROM \" + 'smiles_test' + \"  WHERE \" + 'id' + \" = \" + str(1)\n",
    "# cursor.execute(sqlite_select_Query)\n",
    "# record = cursor.fetchone()\n",
    "# print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/18/2020 14:06:43 - INFO - minGPT.mingpt.model -   number of parameters: 6.382080e+06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINS new version available: upgrade to v0.16.4 is recommended!\n"
     ]
    }
   ],
   "source": [
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=n_layer, n_head=n_head, n_embd=n_embd)\n",
    "model = GPT(mconf)\n",
    "# model.load_state_dict(torch.load(experiment_path + 'model_0.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[:, [-1]]] = -float('Inf')\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_clean(model, xs, steps, temperature=1.0, sample=False, top_k=None, itos = None, stop_token = '$',batch_size = 4):\n",
    "#     block_size = model.get_block_size()\n",
    "    model.eval()\n",
    "    xs = sorted(xs, key = lambda x: x.size(1))\n",
    "    ## add the batch size\n",
    "    xs = [torch.cat(batch_size*[x]) for x in xs]\n",
    "    \n",
    "    ## start by making them the same length\n",
    "    x = xs[0]\n",
    "    for i in range(1,len(xs)): # nothing to do if length 1\n",
    "        while x.size(1) < xs[i].size(1):\n",
    "            x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
    "            logits, _ = model(x_cond)\n",
    "            # pluck the logits at the final step and scale by temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop probabilities to only the top k options\n",
    "            if top_k is not None:\n",
    "                logits = top_k_logits(logits, top_k)\n",
    "            # apply softmax to convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution or take the most likely\n",
    "            if sample:\n",
    "                ix = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "            # append to the sequence and continue\n",
    "            x = torch.cat((x, ix), dim=1)\n",
    "        x= torch.cat((x,xs[i]), dim=0)\n",
    "    \n",
    "    \n",
    "    ## now we can keep going...\n",
    "    keep_going = True\n",
    "    while keep_going:\n",
    "#         print(x.size(1))\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
    "        logits, _ = model(x_cond)\n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "        # append to the sequence and continue\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "        if x.size(1) == block_size:\n",
    "            keep_going = False\n",
    "        if itos is not None:\n",
    "            sequences_over = 0\n",
    "            for i in range(x.size(0)):                \n",
    "                if itos[int(ix[i])] == stop_token:\n",
    "                    sequences_over += 1\n",
    "            if sequences_over == x.size(0):\n",
    "                keep_going = False\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    batch_size = 64\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False\n",
    "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = None\n",
    "    num_workers = 0 # for DataLoader\n",
    "    test_every_iteration = 50000\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_dataset, test_dataset, config):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.config = config\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        logger.info(\"saving %s\", self.config.ckpt_path)\n",
    "        torch.save(raw_model.state_dict(), self.config.ckpt_path + 'model.pt')\n",
    "\n",
    "    def train(self, writer):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "\n",
    "        def run_epoch(split, epoch):\n",
    "            is_train = split == 'train'\n",
    "            model.train(is_train)\n",
    "            data = self.train_dataset if is_train else self.test_dataset\n",
    "            loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
    "                                batch_size=config.batch_size,\n",
    "                                num_workers=config.num_workers)\n",
    "\n",
    "            losses = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "            for it, (x, y) in pbar:\n",
    "\n",
    "                # place data on the correct device\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                # forward the model\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    logits, loss = model(x, y)\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "\n",
    "                if is_train:\n",
    "\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # decay the learning rate based on our progress\n",
    "                    if config.lr_decay:\n",
    "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "                        if self.tokens < config.warmup_tokens:\n",
    "                            # linear warmup\n",
    "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                        else:\n",
    "                            # cosine learning rate decay\n",
    "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                        lr = config.learning_rate * lr_mult\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                    else:\n",
    "                        lr = config.learning_rate\n",
    "\n",
    "                    # report progress\n",
    "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
    "                    \n",
    "                    ## check if iterations is modulo of iteration interval and performa  test:\n",
    "                    \n",
    "                    if (it+1) % config.test_every_iteration == 0 and self.test_dataset is not None:\n",
    "                        test_loss = run_epoch('test', epoch)\n",
    "                        x = (epoch) * len(self.train_dataset) + it * self.config.batch_size\n",
    "                        writer.add_scalar('loss/test_loss',test_loss,x)\n",
    "                        \n",
    "                        MolFormulas = self.config.MolFormulas\n",
    "                        logps = self.config.logps\n",
    "                        df, completions = check_model_output(model, MolFormulas, logps, batch_size = 16)  \n",
    "                        results = check_df(df)\n",
    "                        for i in ['MolFormula_success','MolFormula_failure','smiles_failure_ratio']:\n",
    "                            writer.add_scalar('results/'+i,results[i],x)\n",
    "                        writer.add_scalar('results_logp/'+'logp_mse',results['logp_mse'],x)\n",
    "                        self.save_checkpoint()\n",
    "                    \n",
    "\n",
    "            if not is_train:\n",
    "                test_loss = float(np.mean(losses))\n",
    "                logger.info(\"test loss: %f\", test_loss)\n",
    "                return test_loss\n",
    "            else:\n",
    "                train_loss = float(np.mean(losses))\n",
    "                return train_loss\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "        for epoch in range(config.max_epochs):\n",
    "\n",
    "            train_loss = run_epoch('train', epoch)\n",
    "            writer.add_scalar('loss/train_loss',train_loss,epoch)\n",
    "            if self.test_dataset is not None:\n",
    "                test_loss = run_epoch('test', epoch)\n",
    "                \n",
    "                x = (epoch+1) * len(self.train_dataset)# + it * self.config.batch_size\n",
    "                writer.add_scalar('loss/test_loss',test_loss,x)\n",
    "                MolFormulas = self.config.MolFormulas\n",
    "                logps = self.config.logps\n",
    "                df, completions = check_model_output(model, MolFormulas, logps, batch_size = 16)  \n",
    "                results = check_df(df)\n",
    "                for i in ['MolFormula_success','MolFormula_failure','smiles_failure_ratio']:\n",
    "                    writer.add_scalar('results/'+i,results[i],x)\n",
    "                writer.add_scalar('results_logp/'+'logp_mse',results['logp_mse'],x)\n",
    "            \n",
    "                self.save_checkpoint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=max_epochs, batch_size=batch_size, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(train_dataset)*block_size,\n",
    "                      num_workers=2, ckpt_path = experiment_path,\n",
    "                     test_every_iteration = test_every_iteration, MolFormulas = MolFormulas, logps = logps) ## 10000 * 128 = 1M approx\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "trainer.train(writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, completion = check_model_output(model,['C16H32N4O2'],[1.0],batch_size=16,temperature=2)\n",
    "df = completion_to_pandas(completion)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = 'CCOC(=O)c1ccc(O[C@H]2CCC[C@](C)(OCC)CC2)c(OCC2CCOCC2)c1'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_smiles",
   "language": "python",
   "name": "pytorch_smiles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
